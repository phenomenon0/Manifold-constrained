@article{shazeer2017outrageously,
  title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017},
  url={https://arxiv.org/abs/1701.06538}
}

@article{lepikhin2020gshard,
  title={GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020},
  url={https://arxiv.org/abs/2006.16668}
}

@article{fedus2021switch,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={arXiv preprint arXiv:2101.03961},
  year={2021},
  url={https://arxiv.org/abs/2101.03961}
}

@article{frantar2023qmoe,
  title={QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models},
  author={Frantar, Elias and Alistarh, Dan},
  journal={arXiv preprint arXiv:2310.16795},
  year={2023},
  url={https://arxiv.org/abs/2310.16795}
}

@article{deepseek2024v3,
  title={DeepSeek-V3 Technical Report},
  author={{DeepSeek-AI}},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024},
  url={https://arxiv.org/abs/2412.19437}
}

@article{huang2025mhc,
  title={mHC: Memory-Safe Hierarchical Caching Reduces Meta Data and Eliminates Load Peaks for Dynamic MoE Serving},
  author={Huang, Claus and Chi, Han and Luo, Chenyang and Wei, Xia and An, Bolin and Guo, Wei and Xin, Xin and Xu, Qian and Zhang, Lei},
  journal={arXiv preprint arXiv:2512.24880},
  year={2025},
  url={https://arxiv.org/abs/2512.24880}
}
