\begin{abstract}
This paper studies a concrete architectural claim: sparse expert routing can be
made more interpretable and compression-native by replacing learned router
logits with reconstruction-error routing, then parameterizing experts as
manifold-constrained shared anchors plus quantized deltas.
In MCQSMoE, each expert acts as a compressor-reconstructor, routing is
\emph{top-k lowest reconstruction error}, and expert weights are represented as
\(W_e=\mathcal{R}(A+\Delta_e)\), where \(A\) is a shared anchor and
\(\Delta_e\) is low-bit groupwise quantized.
This couples three practical goals in one design: deterministic routing,
memory-efficient expert storage, and auditable per-token decisions.

On the 2026-02-08 run snapshot, all evaluation gates pass
(Q/QS/P/PS/DB/T/X). Multi-run performance shows
\texttt{ParallelForward\_medium\_b32/Parallel} at 529.7 tok/s versus
100.6 tok/s for serial (5.26x), and \texttt{ParallelZeroCopy} at 512.8 tok/s
(5.10x). Under a matched dense baseline, QSMoE is faster in both tested regimes
(1.252x small, 1.193x large). Trained-artifact parity reports tight numerical
agreement (max absolute error \(\le 7.84\times10^{-4}\), NRMSE \(\approx0.02\%\))
and deterministic replay over 10 runs, while imported FFN artifacts preserve
1.94x memory savings.

These results establish internal algorithmic validity and systems efficiency for
the proposed design. We do not claim public-benchmark state-of-the-art yet; the
external harness remains a small-scale calibration checkpoint.
\end{abstract}
